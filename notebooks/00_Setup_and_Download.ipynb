{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4eb19a9-00a9-41eb-b3ad-acea90c766f5",
   "metadata": {},
   "source": [
    "## MAJOR STEP 1: Metadata Preparation\n",
    "\n",
    "**Goal:** Load, inspect, and clean the raw SRA metadata (`SraRunTable_raw.csv`).\n",
    "**Why:** We need a clean mapping file that connects each **scientific sample ID** (e.g., \"PA011\") to its **technical run ID** (e.g., \"SRR166...\"). This simple, clean file will be the \"driver\" for our entire Snakemake pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b08ee58-b331-493c-8a50-c52597d5e168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- 1. Define Paths ---\n",
    "# (Using '../' to go up from 'notebooks/' directory)\n",
    "file_in = '../data/SraRunTable_raw.csv'\n",
    "file_out = '../results/metadata/metadata_clean.csv'\n",
    "\n",
    "# --- 2. Define Columns ---\n",
    "# (Based on our 'Inspection' step)\n",
    "columns_to_keep = [\n",
    "    'Run',\n",
    "    'BioSample',\n",
    "    'Sample Name'\n",
    "]\n",
    "\n",
    "# (New names for clarity in our pipeline)\n",
    "new_column_names = {\n",
    "    'Run': 'run_id',\n",
    "    'BioSample': 'biosample_id',\n",
    "    'Sample Name': 'sample_id'\n",
    "}\n",
    "\n",
    "# --- 3. Run the Main Step (Read, Clean, Rename) ---\n",
    "print(f\"Reading raw metadata from {file_in}...\")\n",
    "\n",
    "# usecols= : Reads only the columns we need (very efficient)\n",
    "df = pd.read_csv(file_in, usecols=columns_to_keep)\n",
    "\n",
    "# Rename columns to our new standard\n",
    "df = df.rename(columns=new_column_names)\n",
    "\n",
    "# --- 4. Verification (Respecting Rule 1) ---\n",
    "print(f\"Cleaned {len(df)} records. Verifying first 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# --- 5. Save the Clean File ---\n",
    "# (We must create the 'results/metadata' directory first)\n",
    "import os\n",
    "os.makedirs('../results/metadata', exist_ok=True) # Ensure directory exists\n",
    "\n",
    "df.to_csv(file_out, index=False)\n",
    "print(f\"\\nSuccessfully saved clean metadata to {file_out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932e4e6f-3d33-4206-8674-8ba19ed4eabc",
   "metadata": {},
   "source": [
    "##  STEP 2: Test Data Download (Single Sample)\n",
    "\n",
    "**Goal:** Test our download pipeline on a *single* sample.\n",
    "**Why:** Before automating all 96 downloads (the \"High-Throughput\" part), we must verify that our `fasterq-dump` command works correctly and that our `metadata_clean.csv` file provides the correct ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05efd078-0618-4c0d-9e04-1d29d6fb3c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- 1. Load our clean metadata map ---\n",
    "metadata_file = '../results/metadata/metadata_clean.csv'\n",
    "df_meta = pd.read_csv(metadata_file)\n",
    "\n",
    "# --- 2. Select our single test sample ---\n",
    "# We'll just pick the first one from the file\n",
    "test_run_id = df_meta.loc[0, 'run_id']\n",
    "test_sample_id = df_meta.loc[0, 'sample_id']\n",
    "\n",
    "print(f\"--- Preparing to test download for: ---\")\n",
    "print(f\"Sample ID: {test_sample_id}\")\n",
    "print(f\"Run ID: {test_run_id}\")\n",
    "\n",
    "# --- 3. Define and create output directory ---\n",
    "output_dir = '../data/raw_reads'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Ensured output directory exists: {output_dir}\")\n",
    "\n",
    "# --- 4. Build and run the fasterq-dump command ---\n",
    "# We use:\n",
    "# --split-files : To get R1 and R2 (this is Paired-End data)\n",
    "# -O : Output directory\n",
    "# -p : Show progress\n",
    "\n",
    "command = f\"fasterq-dump --split-files -O {output_dir} -p {test_run_id}\"\n",
    "\n",
    "print(f\"\\nExecuting command:\\n{command}\")\n",
    "\n",
    "# This will run the command in the shell. It may take a minute.\n",
    "!{command}\n",
    "\n",
    "# --- 5. Verification (Rule 1) ---\n",
    "# If this step succeeds, we should see the new FASTQ files\n",
    "print(f\"\\n--- Verification: Listing files in {output_dir} ---\")\n",
    "!ls -lh {output_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a377b8dc-fabc-49fd-95ba-e0f62cce71fa",
   "metadata": {},
   "source": [
    "##  Conclusion & Handoff to Automation\n",
    "\n",
    "**Status:** Success. We have confirmed two critical things:\n",
    "1.  Our metadata is clean and saved (`results/metadata/metadata_clean.csv`).\n",
    "2.  Our download tool (`fasterq-dump`) and method are working correctly on a single test sample.\n",
    "\n",
    "**Next Step (The Handoff):**\n",
    "The R&D (Research & Development) phase in this notebook is complete. The \"Production\" phase (downloading all 96 samples) is now automated using the main project `Snakefile`.\n",
    "\n",
    "To execute the full, high-throughput download, run the following command **from the main project terminal**:\n",
    "\n",
    "```bash\n",
    "snakemake --cores 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
